{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-13T12:07:34.977154Z",
     "start_time": "2024-12-13T12:07:33.605943Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingRegressor, HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Load the datasets\n",
    "awards_players_df = pd.read_csv('../dataset/awards_players.csv')\n",
    "coaches_df = pd.read_csv('../dataset/coaches.csv')\n",
    "players_df = pd.read_csv('../dataset/players.csv')\n",
    "players_teams_df = pd.read_csv('../dataset/players_teams.csv')\n",
    "series_post_df = pd.read_csv('../dataset/series_post.csv')\n",
    "teams_df = pd.read_csv('../dataset/teams.csv')\n",
    "teams_post_df = pd.read_csv('../dataset/teams_post.csv')\n",
    "\n",
    "# Remove useless columns from the datasets\n",
    "awards_players_df = awards_players_df.drop(columns=['lgID'])\n",
    "players_df = players_df.drop(columns=['firstseason', 'lastseason', 'deathDate'])\n",
    "coaches_df = coaches_df.drop(columns=['lgID'])\n",
    "series_post_df = series_post_df.drop(columns=['lgIDLoser', 'lgIDWinner'])\n",
    "teams_post_df = teams_post_df.drop(columns=['lgID'])\n",
    "teams_df = teams_df.drop(\n",
    "    columns=['lgID', 'divID', 'seeded', 'tmORB', 'tmDRB', 'tmTRB', 'opptmORB', 'opptmDRB', 'opptmTRB'])\n",
    "players_teams_df = players_teams_df.drop(columns=['lgID'])"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T12:07:35.006587Z",
     "start_time": "2024-12-13T12:07:34.989583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Merge players, teams, and awards data\n",
    "players_teams_merged = pd.merge(players_df, players_teams_df, left_on='bioID', right_on='playerID')\n",
    "players_teams_awards = pd.merge(players_teams_merged, awards_players_df, on=['year', 'playerID'], how='left')\n",
    "\n",
    "#remove pos, height, weight, college, collegeOther, birthDate, playerID, GP and GS\n",
    "players_teams_awards = players_teams_awards.drop(\n",
    "    columns=['pos', 'height', 'weight', 'college', 'collegeOther', 'birthDate', 'playerID', 'GP', 'GS', 'PostGP', 'PostGS'])\n"
   ],
   "id": "80171f90bfca7281",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T12:07:35.134244Z",
     "start_time": "2024-12-13T12:07:35.110564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define award scores\n",
    "award_scores = {\n",
    "    'All-Star Game Most Valuable Player': 7,\n",
    "    'Coach of the Year': 10,\n",
    "    'Defensive Player of the Year': 7,\n",
    "    'Kim Perrot Sportsmanship Award': 0,\n",
    "    'Most Improved Player': 5,\n",
    "    'Most Valuable Player': 10,\n",
    "    'Rookie of the Year': 5,\n",
    "    'Sixth Woman of the Year': 6,\n",
    "    'WNBA Finals Most Valuable Player': 8,\n",
    "    'WNBA All-Decade Team': 6,\n",
    "    'WNBA All Decade Team Honorable Mention': 4\n",
    "}\n",
    "\n",
    "# Map the award scores to the dataframe\n",
    "players_teams_awards['award_score'] = players_teams_awards['award'].map(award_scores).fillna(0)\n",
    "\n",
    "# drop award column\n",
    "players_teams_awards = players_teams_awards.drop(columns=['award'])\n",
    "\n",
    "# List of columns to group by (excluding 'award_score')\n",
    "columns_to_group_by = ['bioID', 'year', 'stint']\n",
    "\n",
    "# Group by the columns and aggregate\n",
    "players_teams_awards = players_teams_awards.groupby(columns_to_group_by).agg({\n",
    "    'award_score': 'sum',  # Sum award scores\n",
    "    **{col: 'first' for col in players_teams_awards.columns if col not in columns_to_group_by + ['award_score']}\n",
    "}).reset_index()"
   ],
   "id": "7d44721cf4b465d2",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T12:07:35.333754Z",
     "start_time": "2024-12-13T12:07:35.194218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define coefficients for player statistics\n",
    "coefficients = {\n",
    "    'minutes': 0.2,\n",
    "    'points': 0.45,\n",
    "    'oRebounds': 0.1,\n",
    "    'dRebounds': 0.1,\n",
    "    'rebounds': 0.15,\n",
    "    'assists': 0.25,\n",
    "    'steals': 0.2,\n",
    "    'blocks': 0.35,\n",
    "    'turnovers': -0.3,  # Negative coefficient for turnovers\n",
    "    'PF': 0.1,\n",
    "    'fgAttempted': 0.05,\n",
    "    'fgMade': 0.1,\n",
    "    'ftAttempted': 0.1,\n",
    "    'ftMade': 0.2,\n",
    "    'threeAttempted': 0.15,\n",
    "    'threeMade': 0.25,\n",
    "    'dq': -0.4  # Negative coefficient for dq\n",
    "}\n",
    "\n",
    "# List of columns to be used in the weighted sum calculation\n",
    "columns_to_use = list(coefficients.keys())\n",
    "\n",
    "\n",
    "# Function to calculate the weighted sum based on coefficients\n",
    "def calculate_weighted_sum(row):\n",
    "    total = 0\n",
    "    for col in columns_to_use:\n",
    "        total += row[col] * coefficients[col]\n",
    "    return total\n",
    "\n",
    "\n",
    "# Apply the function to calculate the weighted sum and store it in a new column\n",
    "players_teams_awards['weighted_score'] = players_teams_awards.apply(calculate_weighted_sum, axis=1)\n",
    "\n",
    "# Define coefficients for post-season statistics\n",
    "post_coefficients = {\n",
    "    'PostMinutes': 0.2,\n",
    "    'PostPoints': 0.45,\n",
    "    'PostoRebounds': 0.1,\n",
    "    'PostdRebounds': 0.1,\n",
    "    'PostRebounds': 0.15,\n",
    "    'PostAssists': 0.25,\n",
    "    'PostSteals': 0.2,\n",
    "    'PostBlocks': 0.35,\n",
    "    'PostTurnovers': -0.3,  # Negative coefficient for turnovers\n",
    "    'PostPF': 0.1,\n",
    "    'PostfgAttempted': 0.05,\n",
    "    'PostfgMade': 0.1,\n",
    "    'PostftAttempted': 0.1,\n",
    "    'PostftMade': 0.2,\n",
    "    'PostthreeAttempted': 0.15,\n",
    "    'PostthreeMade': 0.25,\n",
    "    'PostDQ': -0.4  # Negative coefficient for dq\n",
    "}\n",
    "\n",
    "# List of 'Post' columns to be used in the weighted sum calculation\n",
    "post_columns_to_use = list(post_coefficients.keys())\n",
    "\n",
    "\n",
    "# Function to calculate the weighted sum based on 'Post' coefficients\n",
    "def calculate_post_weighted_sum(row):\n",
    "    total = 0\n",
    "    for col in post_columns_to_use:\n",
    "        total += row[col] * post_coefficients[col]\n",
    "    return total\n",
    "\n",
    "\n",
    "# Apply the function to calculate the post-season weighted sum and store it in a new column\n",
    "players_teams_awards['post_weighted_score'] = players_teams_awards.apply(calculate_post_weighted_sum, axis=1)\n",
    "\n",
    "# Remove the individual columns used in the calculation\n",
    "players_teams_awards.drop(columns=columns_to_use, inplace=True)\n",
    "\n",
    "# Remove the individual 'Post' columns used in the calculation\n",
    "players_teams_awards.drop(columns=post_columns_to_use, inplace=True)\n",
    "\n",
    "# add the franchID column to the players_teams_awards (check the tmID and year and add it)\n",
    "players_teams_awards = pd.merge(players_teams_awards, teams_df[['year', 'tmID', 'franchID']], on=['year', 'tmID'], how='left')"
   ],
   "id": "e83510449d90487e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T12:07:35.347074Z",
     "start_time": "2024-12-13T12:07:35.341301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create a clean_teams df that only has year, tmID, franchID, and playoff, and map Y to 1 and N to 0 in playoff\n",
    "clean_teams = teams_df[['year', 'tmID', 'confID', 'playoff']].copy()\n",
    "clean_teams['playoff'] = clean_teams['playoff'].map({'Y': 1, 'N': 0})"
   ],
   "id": "f7ffcfbbe5d0ff68",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T12:07:35.852466Z",
     "start_time": "2024-12-13T12:07:35.423308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# look at tmID to calculate the columns of the team statistics when merging with players_teams_awards, but look at the franchID to compare the rolling average of the team statistics\n",
    "\n",
    "def calculate_rolling_features(df, columns, window=3):\n",
    "    \"\"\"\n",
    "    Calculates rolling average features for the given columns in the dataframe,\n",
    "    handling duplicates and considering 'stint' as part of the aggregation.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe.\n",
    "        columns (list of str): List of column names to calculate rolling features for.\n",
    "        window (int): Rolling window size. Default is 3.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with added rolling features, including tmID and franchID.\n",
    "    \"\"\"\n",
    "    # Keep tmID and franchID for merging later\n",
    "    id_columns = ['bioID', 'year', 'tmID', 'franchID']\n",
    "\n",
    "    # Aggregate duplicate rows for the same bioID, year pair\n",
    "    aggregated_df = (\n",
    "        df.groupby(['bioID', 'year'])[columns]\n",
    "        .sum()  # Sum scores across stints\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Add tmID and franchID back after aggregating\n",
    "    unique_id_data = df[id_columns].drop_duplicates(subset=['bioID', 'year'])\n",
    "    aggregated_df = aggregated_df.merge(unique_id_data, on=['bioID', 'year'], how='left')\n",
    "\n",
    "    # Sort by bioID and year\n",
    "    aggregated_df = aggregated_df.sort_values(['bioID', 'year']).copy()\n",
    "\n",
    "    for col in columns:\n",
    "        rolling_col_name = f'{col}_rolling_{window}'\n",
    "\n",
    "        def rolling_avg(group):\n",
    "            values = group.shift(1)  # Exclude current season by shifting\n",
    "            filtered = values.replace(0, np.nan)  # Replace zeros with NaN\n",
    "            return (\n",
    "                filtered.rolling(window=window, min_periods=1)\n",
    "                .mean()  # Calculate rolling mean, ignoring NaN\n",
    "            )\n",
    "\n",
    "        aggregated_df[rolling_col_name] = (\n",
    "            aggregated_df.groupby('bioID')[col]\n",
    "            .apply(rolling_avg)\n",
    "            .reset_index(level=0, drop=True)  # Align index with aggregated_df\n",
    "        )\n",
    "\n",
    "    return aggregated_df\n",
    "\n",
    "# Columns to calculate rolling features for\n",
    "rolling_columns = ['award_score', 'weighted_score', 'post_weighted_score']\n",
    "\n",
    "player_rolling_features = calculate_rolling_features(\n",
    "    players_teams_awards,\n",
    "    columns=rolling_columns\n",
    ")\n",
    "\n",
    "# replace NaN values with 0\n",
    "player_rolling_features = player_rolling_features.fillna(0)"
   ],
   "id": "3d28c5a4e0e85201",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T12:07:35.949836Z",
     "start_time": "2024-12-13T12:07:35.894568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate Regular Season Win Percentage\n",
    "coaches_df['win_percentage'] = (coaches_df['won'] / (coaches_df['won'] + coaches_df['lost'])) * 100\n",
    "\n",
    "# Calculate Postseason Win Percentage\n",
    "coaches_df['post_win_percentage'] = (coaches_df['post_wins'] / (coaches_df['post_wins'] + coaches_df['post_losses']).replace(0, pd.NA)) * 100\n",
    "\n",
    "# fill NaN values with 0\n",
    "coaches_df = coaches_df.fillna(0)\n",
    "\n",
    "# Display the DataFrame with the new columns\n",
    "print(coaches_df)\n",
    "\n",
    "def calculate_rolling_features_coaches(df, columns, window=3):\n",
    "    \"\"\"\n",
    "    Calculates rolling average features for the given columns in the coaches dataframe,\n",
    "    handling duplicates and keeping 'stint' as part of the aggregation.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe with coach data.\n",
    "        columns (list of str): List of column names to calculate rolling features for.\n",
    "        window (int): Rolling window size. Default is 3.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with added rolling features and 'stint' included.\n",
    "    \"\"\"\n",
    "    # Keep coachID, year, stint, tmID for merging later\n",
    "    id_columns = ['coachID', 'year', 'stint', 'tmID']\n",
    "\n",
    "    # Aggregate duplicate rows for the same coachID, year pair, averaging the specified columns\n",
    "    aggregated_df = (\n",
    "        df.groupby(['coachID', 'year', 'stint'])[columns]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Add additional ID columns back after aggregating\n",
    "    unique_id_data = df[id_columns].drop_duplicates(subset=['coachID', 'year', 'stint'])\n",
    "    aggregated_df = aggregated_df.merge(unique_id_data, on=['coachID', 'year', 'stint'], how='left')\n",
    "\n",
    "    # Sort by coachID, year, and stint\n",
    "    aggregated_df = aggregated_df.sort_values(['coachID', 'year', 'stint']).copy()\n",
    "\n",
    "    for col in columns:\n",
    "        rolling_col_name = f'{col}_rolling_{window}'\n",
    "\n",
    "        def rolling_avg(group):\n",
    "            values = group.shift(1)  # Exclude current season by shifting\n",
    "            filtered = values.replace(0, np.nan)  # Replace zeros with NaN\n",
    "            return (\n",
    "                filtered.rolling(window=window, min_periods=1)\n",
    "                .mean()  # Calculate rolling mean, ignoring NaN\n",
    "            )\n",
    "\n",
    "        aggregated_df[rolling_col_name] = (\n",
    "            aggregated_df.groupby('coachID')[col]\n",
    "            .apply(rolling_avg)\n",
    "            .reset_index(level=0, drop=True)  # Align index with aggregated_df\n",
    "        )\n",
    "\n",
    "    return aggregated_df\n",
    "\n",
    "# get the coaches rolling averages\n",
    "coaches_rolling_features = calculate_rolling_features_coaches(coaches_df, ['win_percentage', 'post_win_percentage'])\n",
    "\n",
    "# replace NaN values with 0\n",
    "coaches_rolling_features = coaches_rolling_features.fillna(0)\n",
    "\n",
    "# sort by tmID and year\n",
    "coaches_rolling_features = coaches_rolling_features.sort_values(['tmID', 'year'])"
   ],
   "id": "f9f0b905e22fa4f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        coachID  year tmID  stint  won  lost  post_wins  post_losses  \\\n",
      "0    adamsmi01w     5  WAS      0   17    17          1            2   \n",
      "1    adubari99w     1  NYL      0   20    12          4            3   \n",
      "2    adubari99w     2  NYL      0   21    11          3            3   \n",
      "3    adubari99w     3  NYL      0   18    14          4            4   \n",
      "4    adubari99w     4  NYL      0   16    18          0            0   \n",
      "..          ...   ...  ...    ...  ...   ...        ...          ...   \n",
      "157  wintebr01w     6  IND      0   21    13          2            2   \n",
      "158  wintebr01w     7  IND      0   21    13          0            2   \n",
      "159  wintebr01w     8  IND      0   21    13          3            3   \n",
      "160  zierddo99w     8  MIN      0   10    24          0            0   \n",
      "161  zierddo99w     9  MIN      0   16    18          0            0   \n",
      "\n",
      "     win_percentage  post_win_percentage  \n",
      "0         50.000000            33.333333  \n",
      "1         62.500000            57.142857  \n",
      "2         65.625000            50.000000  \n",
      "3         56.250000            50.000000  \n",
      "4         47.058824             0.000000  \n",
      "..              ...                  ...  \n",
      "157       61.764706            50.000000  \n",
      "158       61.764706             0.000000  \n",
      "159       61.764706            50.000000  \n",
      "160       29.411765             0.000000  \n",
      "161       47.058824             0.000000  \n",
      "\n",
      "[162 rows x 10 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_136032/1802900604.py:8: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  coaches_df = coaches_df.fillna(0)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T12:07:35.998297Z",
     "start_time": "2024-12-13T12:07:35.976226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Merge with clean_teams on year and tmID\n",
    "merged_df = pd.merge(clean_teams, player_rolling_features, on=['year', 'tmID'], how='left')\n",
    "\n",
    "# Merge with coaches_rolling_features on year and tmID, in the case of multiple stints for the coach, keep the second stint\n",
    "# merged_df = pd.merge(merged_df, coaches_rolling_features, on=['year', 'tmID'], how='left')\n",
    "\n",
    "# Aggregate team-level statistics by year and tmID\n",
    "teams_with_rolling_aggregated = merged_df.groupby(['year', 'tmID'], as_index=False).agg({\n",
    "    'franchID': 'first',  # Keep the first franchID\n",
    "    'playoff': 'first',   # Keep the first playoff value\n",
    "    'confID': 'first',\n",
    "    'award_score_rolling_3': 'sum',\n",
    "    'weighted_score_rolling_3': 'sum',\n",
    "    'post_weighted_score_rolling_3': 'sum',\n",
    "})\n",
    "\n",
    "# Define the rolling columns for features\n",
    "rolling_columns_aggregated = [\n",
    "    'award_score_rolling_3',\n",
    "    'weighted_score_rolling_3',\n",
    "    'post_weighted_score_rolling_3'\n",
    "]\n",
    "\n",
    "# Filter data for training and testing\n",
    "train_data = teams_with_rolling_aggregated[\n",
    "    (teams_with_rolling_aggregated['year'] >= 4) & (teams_with_rolling_aggregated['year'] <= 8)\n",
    "    ]\n",
    "test_data = teams_with_rolling_aggregated[teams_with_rolling_aggregated['year'] == 9]\n",
    "\n",
    "# divide train_data into separate files for each conference\n",
    "train_data_east = train_data[train_data['confID'] == 'EA']\n",
    "train_data_west = train_data[train_data['confID'] == 'WE']\n",
    "\n",
    "# divide test_data into separate files for each conference\n",
    "test_data_east = test_data[test_data['confID'] == 'EA']\n",
    "test_data_west = test_data[test_data['confID'] == 'WE']\n",
    "\n",
    "# Prepare features and labels\n",
    "X_train_east = train_data_east[rolling_columns_aggregated].fillna(0)\n",
    "y_train_east = train_data_east['playoff']\n",
    "\n",
    "X_train_west = train_data_west[rolling_columns_aggregated].fillna(0)\n",
    "y_train_west = train_data_west['playoff']\n",
    "\n",
    "X_test_east = test_data_east[rolling_columns_aggregated].fillna(0)\n",
    "y_test_east = test_data_east['playoff']\n",
    "\n",
    "X_test_west = test_data_west[rolling_columns_aggregated].fillna(0)\n",
    "y_test_west = test_data_west['playoff']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_west_scaled = scaler.fit_transform(X_train_west)\n",
    "X_test_west_scaled = scaler.transform(X_test_west)\n",
    "X_train_east_scaled = scaler.fit_transform(X_train_east)\n",
    "X_test_east_scaled = scaler.transform(X_test_east)"
   ],
   "id": "3d67736bad516c3e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T12:07:36.063149Z",
     "start_time": "2024-12-13T12:07:36.060613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Constants for playoff spots\n",
    "PLAYOFF_SPOTS = 8  # Total across both conferences\n",
    "PLAYOFF_SPOTS_PER_CONF = 4  # per conference"
   ],
   "id": "61e2969eaf87194b",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T12:07:36.245804Z",
     "start_time": "2024-12-13T12:07:36.123517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Softmax function to normalize the probabilities\n",
    "def apply_softmax(df):\n",
    "    # Compute the softmax for probabilities\n",
    "    exp_values = np.exp(df['proba'])  # Subtract max for numerical stability\n",
    "    softmax_values = exp_values / exp_values.sum()\n",
    "    return softmax_values\n",
    "\n",
    "# Train Logistic Regression model for the West conference\n",
    "clf_west = RandomForestClassifier(random_state=42)\n",
    "clf_west.fit(X_train_west_scaled, y_train_west)\n",
    "\n",
    "# Predict probabilities for class 1 (making playoffs) for the West\n",
    "y_proba_west = clf_west.predict_proba(X_test_west_scaled)[:, 1]\n",
    "\n",
    "# Prepare predictions for the West\n",
    "predictions_west = test_data_west[['year', 'tmID', 'franchID', 'confID']].copy()\n",
    "predictions_west[rolling_columns_aggregated] = X_test_west_scaled  # Assign scaled features correctly\n",
    "predictions_west['proba'] = y_proba_west  # Predicted probabilities\n",
    "predictions_west['true_label'] = y_test_west.values  # True labels\n",
    "\n",
    "# Apply softmax to the West conference predictions\n",
    "predictions_west['softmax_proba'] = apply_softmax(predictions_west)\n",
    "\n",
    "# Sort by year, confID, and probability, descending\n",
    "predictions_west = predictions_west.sort_values(by=['year', 'confID', 'softmax_proba'], ascending=[True, True, False])\n",
    "\n",
    "# Apply playoff cutoff per conference and year\n",
    "final_predictions_west = []\n",
    "\n",
    "for (year, confID), group in predictions_west.groupby(['year', 'confID']):\n",
    "    group['playoff_pred'] = 0  # Default to not making playoffs\n",
    "    group.loc[group.head(PLAYOFF_SPOTS_PER_CONF).index, 'playoff_pred'] = 1  # Top 4 in each conference\n",
    "    final_predictions_west.append(group)\n",
    "\n",
    "# Combine results for the West conference\n",
    "final_predictions_west = pd.concat(final_predictions_west)\n"
   ],
   "id": "67f742d27d305b32",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T12:07:36.515074Z",
     "start_time": "2024-12-13T12:07:36.382132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train Logistic Regression model for the East conference\n",
    "clf_east = RandomForestClassifier(random_state=42)\n",
    "clf_east.fit(X_train_east_scaled, y_train_east)\n",
    "\n",
    "# Predict probabilities for class 1 (making playoffs) for the East\n",
    "y_proba_east = clf_east.predict_proba(X_test_east_scaled)[:, 1]\n",
    "\n",
    "# Prepare predictions for the East\n",
    "predictions_east = test_data_east[['year', 'tmID', 'franchID', 'confID']].copy()\n",
    "predictions_east[rolling_columns_aggregated] = X_test_east_scaled  # Add the rolling features\n",
    "predictions_east['proba'] = y_proba_east  # Predicted probabilities\n",
    "predictions_east['true_label'] = y_test_east.values\n",
    "\n",
    "# Apply softmax to the East conference predictions\n",
    "print(\"THIS IS BEFORE SOFTMAX\")\n",
    "print(predictions_east)\n",
    "predictions_east['softmax_proba'] = apply_softmax(predictions_east)\n",
    "print(\"THIS IS AFTER SOFTMAX\")\n",
    "print(predictions_east)\n",
    "\n",
    "# Sort by year, confID, and probability, descending\n",
    "predictions_east = predictions_east.sort_values(by=['year', 'confID', 'softmax_proba'], ascending=[True, True, False])\n",
    "\n",
    "# Apply playoff cutoff per conference and year\n",
    "final_predictions_east = []\n",
    "\n",
    "for (year, confID), group in predictions_east.groupby(['year', 'confID']):\n",
    "    group['playoff_pred'] = 0  # Default to not making playoffs\n",
    "    group.loc[group.head(PLAYOFF_SPOTS_PER_CONF).index, 'playoff_pred'] = 1  # Top 4 in each conference\n",
    "    final_predictions_east.append(group)\n",
    "\n",
    "# Combine results for the East conference\n",
    "final_predictions_east = pd.concat(final_predictions_east)\n",
    "\n",
    "# Combine both East and West predictions\n",
    "final_predictions = pd.concat([final_predictions_west, final_predictions_east])\n",
    "\n",
    "# Ensure output is ordered by year and franchise ID\n",
    "final_predictions = final_predictions.sort_values(by=['year', 'franchID'])\n",
    "\n",
    "# Generate probabilities for playoffs per team, ordered by year and franchise\n",
    "playoff_probs = final_predictions[['year', 'tmID', 'franchID', 'softmax_proba', 'true_label']]\n",
    "\n",
    "# nyltiply softmax_proba by 4\n",
    "playoff_probs['softmax_proba'] = playoff_probs['softmax_proba'] * 4\n",
    "\n",
    "# Example output: Top probabilities for teams in year 9\n",
    "example_year = 9\n",
    "example_probs = playoff_probs[playoff_probs['year'] == example_year].sort_values(by='softmax_proba', ascending=False)\n",
    "print(example_probs)\n",
    "\n",
    "# Extract final predictions and true labels\n",
    "y_pred_final = final_predictions['playoff_pred']\n",
    "y_true_final = final_predictions['true_label']\n",
    "\n",
    "# Evaluate performance metrics\n",
    "print(\"Precision:\", precision_score(y_true_final, y_pred_final))\n",
    "print(\"Recall:\", recall_score(y_true_final, y_pred_final))\n",
    "print(\"Accuracy:\", accuracy_score(y_true_final, y_pred_final))\n",
    "print(\"F1 Score:\", f1_score(y_true_final, y_pred_final))\n",
    "print(classification_report(y_true_final, y_pred_final))\n",
    "\n",
    "# Full probabilities for a specific year (e.g., year 9)\n",
    "playoff_probs_year_9 = playoff_probs[playoff_probs['year'] == example_year]\n",
    "print(playoff_probs_year_9)\n"
   ],
   "id": "c5966910cb7c2cd0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THIS IS BEFORE SOFTMAX\n",
      "     year tmID franchID confID  award_score_rolling_3  \\\n",
      "115     9  ATL      ATL     EA              -0.966591   \n",
      "116     9  CHI      CHI     EA               0.114909   \n",
      "117     9  CON      CON     EA              -0.966591   \n",
      "118     9  DET      DET     EA               4.873513   \n",
      "120     9  IND      IND     EA               2.710511   \n",
      "123     9  NYL      NYL     EA               0.114909   \n",
      "128     9  WAS      WAS     EA              -0.966591   \n",
      "\n",
      "     weighted_score_rolling_3  post_weighted_score_rolling_3  proba  \\\n",
      "115                 -1.483954                      -0.954512   0.11   \n",
      "116                 -0.549526                      -1.424435   0.44   \n",
      "117                 -1.125536                      -0.429899   0.17   \n",
      "118                  0.659437                       1.861288   0.87   \n",
      "120                  1.948197                      -0.113309   0.94   \n",
      "123                 -1.074024                      -0.738963   0.47   \n",
      "128                 -0.038123                      -0.853424   0.42   \n",
      "\n",
      "     true_label  \n",
      "115           0  \n",
      "116           0  \n",
      "117           1  \n",
      "118           1  \n",
      "120           1  \n",
      "123           1  \n",
      "128           0  \n",
      "THIS IS AFTER SOFTMAX\n",
      "     year tmID franchID confID  award_score_rolling_3  \\\n",
      "115     9  ATL      ATL     EA              -0.966591   \n",
      "116     9  CHI      CHI     EA               0.114909   \n",
      "117     9  CON      CON     EA              -0.966591   \n",
      "118     9  DET      DET     EA               4.873513   \n",
      "120     9  IND      IND     EA               2.710511   \n",
      "123     9  NYL      NYL     EA               0.114909   \n",
      "128     9  WAS      WAS     EA              -0.966591   \n",
      "\n",
      "     weighted_score_rolling_3  post_weighted_score_rolling_3  proba  \\\n",
      "115                 -1.483954                      -0.954512   0.11   \n",
      "116                 -0.549526                      -1.424435   0.44   \n",
      "117                 -1.125536                      -0.429899   0.17   \n",
      "118                  0.659437                       1.861288   0.87   \n",
      "120                  1.948197                      -0.113309   0.94   \n",
      "123                 -1.074024                      -0.738963   0.47   \n",
      "128                 -0.038123                      -0.853424   0.42   \n",
      "\n",
      "     true_label  softmax_proba  \n",
      "115           0       0.093623  \n",
      "116           0       0.130226  \n",
      "117           1       0.099412  \n",
      "118           1       0.200191  \n",
      "120           1       0.214707  \n",
      "123           1       0.134192  \n",
      "128           0       0.127648  \n",
      "     year tmID franchID  softmax_proba  true_label\n",
      "120     9  IND      IND       0.858828           1\n",
      "118     9  DET      DET       0.800766           1\n",
      "124     9  PHO      PHO       0.675047           0\n",
      "121     9  LAS      LAS       0.668330           1\n",
      "127     9  SEA      SEA       0.635736           1\n",
      "125     9  SAC      SAC       0.616947           1\n",
      "126     9  SAS      SAS       0.586858           1\n",
      "123     9  NYL      NYL       0.536769           1\n",
      "116     9  CHI      CHI       0.520906           0\n",
      "128     9  WAS      WAS       0.510591           0\n",
      "119     9  HOU      HOU       0.439124           0\n",
      "117     9  CON      CON       0.397649           1\n",
      "122     9  MIN      MIN       0.377958           0\n",
      "115     9  ATL      ATL       0.374491           0\n",
      "Precision: 0.75\n",
      "Recall: 0.75\n",
      "Accuracy: 0.7142857142857143\n",
      "F1 Score: 0.75\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67         6\n",
      "           1       0.75      0.75      0.75         8\n",
      "\n",
      "    accuracy                           0.71        14\n",
      "   macro avg       0.71      0.71      0.71        14\n",
      "weighted avg       0.71      0.71      0.71        14\n",
      "\n",
      "     year tmID franchID  softmax_proba  true_label\n",
      "115     9  ATL      ATL       0.374491           0\n",
      "116     9  CHI      CHI       0.520906           0\n",
      "117     9  CON      CON       0.397649           1\n",
      "118     9  DET      DET       0.800766           1\n",
      "119     9  HOU      HOU       0.439124           0\n",
      "120     9  IND      IND       0.858828           1\n",
      "121     9  LAS      LAS       0.668330           1\n",
      "122     9  MIN      MIN       0.377958           0\n",
      "123     9  NYL      NYL       0.536769           1\n",
      "124     9  PHO      PHO       0.675047           0\n",
      "125     9  SAC      SAC       0.616947           1\n",
      "126     9  SAS      SAS       0.586858           1\n",
      "127     9  SEA      SEA       0.635736           1\n",
      "128     9  WAS      WAS       0.510591           0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_136032/3999868697.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  playoff_probs['softmax_proba'] = playoff_probs['softmax_proba'] * 4\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T12:07:36.656094Z",
     "start_time": "2024-12-13T12:07:36.614248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Combine predictions for both conferences (East and West)\n",
    "predictions_combined = pd.concat([final_predictions, final_predictions_east])\n",
    "\n",
    "# Sort by year, confID, and original probability for softmax scaling\n",
    "predictions_combined = predictions_combined.sort_values(by=['year', 'confID', 'proba'], ascending=[True, True, False])\n",
    "\n",
    "# Apply softmax across both conferences\n",
    "predictions_combined['softmax_proba'] = predictions_combined.groupby(['year', 'confID'])['proba'].transform(lambda x: softmax(x))\n",
    "\n",
    "# Apply playoff cutoff per conference\n",
    "final_predictions_combined = []\n",
    "\n",
    "for (year, confID), group in predictions_combined.groupby(['year', 'confID']):\n",
    "    group['playoff_pred'] = 0  # Default to not making playoffs\n",
    "    # Sort within the group by softmax probabilities and select the top teams\n",
    "    group = group.sort_values(by='softmax_proba', ascending=False)\n",
    "    group.loc[group.head(PLAYOFF_SPOTS_PER_CONF).index, 'playoff_pred'] = 1  # Top 4 in each conference\n",
    "    final_predictions_combined.append(group)\n",
    "\n",
    "# Combine results\n",
    "final_predictions_combined = pd.concat(final_predictions_combined)\n",
    "\n",
    "# Ensure output is ordered by year and franchise ID\n",
    "final_predictions_combined = final_predictions_combined.sort_values(by=['year', 'franchID'])\n",
    "\n",
    "# Evaluate performance metrics for the combined predictions\n",
    "y_pred_combined = final_predictions_combined['playoff_pred']\n",
    "y_true_combined = final_predictions_combined['true_label']\n",
    "\n",
    "# Evaluate predictions\n",
    "print(\"Combined Metrics:\")\n",
    "print(\"Precision:\", precision_score(y_true_combined, y_pred_combined))\n",
    "print(\"Recall:\", recall_score(y_true_combined, y_pred_combined))\n",
    "print(\"Accuracy:\", accuracy_score(y_true_combined, y_pred_combined))\n",
    "print(\"F1 Score:\", f1_score(y_true_combined, y_pred_combined))\n",
    "print(classification_report(y_true_combined, y_pred_combined))\n",
    "\n",
    "# Example output: Top probabilities for all teams in year 9\n",
    "example_year = 9\n",
    "example_probs_combined = final_predictions_combined[final_predictions_combined['year'] == example_year].sort_values(by='softmax_proba', ascending=False)\n",
    "print(example_probs_combined)"
   ],
   "id": "af45fb00d6ede3a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Metrics:\n",
      "Precision: 0.875\n",
      "Recall: 0.5833333333333334\n",
      "Accuracy: 0.7142857142857143\n",
      "F1 Score: 0.7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.89      0.73         9\n",
      "           1       0.88      0.58      0.70        12\n",
      "\n",
      "    accuracy                           0.71        21\n",
      "   macro avg       0.75      0.74      0.71        21\n",
      "weighted avg       0.76      0.71      0.71        21\n",
      "\n",
      "     year tmID franchID confID  award_score_rolling_3  \\\n",
      "124     9  PHO      PHO     WE               0.215099   \n",
      "121     9  LAS      LAS     WE               1.139933   \n",
      "127     9  SEA      SEA     WE               2.551521   \n",
      "125     9  SAC      SAC     WE              -0.563708   \n",
      "126     9  SAS      SAS     WE              -0.953112   \n",
      "119     9  HOU      HOU     WE              -0.369006   \n",
      "120     9  IND      IND     EA               2.710511   \n",
      "120     9  IND      IND     EA               2.710511   \n",
      "118     9  DET      DET     EA               4.873513   \n",
      "118     9  DET      DET     EA               4.873513   \n",
      "122     9  MIN      MIN     WE              -0.466357   \n",
      "123     9  NYL      NYL     EA               0.114909   \n",
      "123     9  NYL      NYL     EA               0.114909   \n",
      "116     9  CHI      CHI     EA               0.114909   \n",
      "116     9  CHI      CHI     EA               0.114909   \n",
      "128     9  WAS      WAS     EA              -0.966591   \n",
      "128     9  WAS      WAS     EA              -0.966591   \n",
      "117     9  CON      CON     EA              -0.966591   \n",
      "117     9  CON      CON     EA              -0.966591   \n",
      "115     9  ATL      ATL     EA              -0.966591   \n",
      "115     9  ATL      ATL     EA              -0.966591   \n",
      "\n",
      "     weighted_score_rolling_3  post_weighted_score_rolling_3  proba  \\\n",
      "124                  0.002833                       2.260649   0.78   \n",
      "121                  0.754968                       0.263389   0.77   \n",
      "127                  0.329393                       0.327565   0.72   \n",
      "125                 -0.983571                       0.853035   0.69   \n",
      "126                 -0.817816                       0.081660   0.64   \n",
      "119                 -0.361897                      -0.056489   0.35   \n",
      "120                  1.948197                      -0.113309   0.94   \n",
      "120                  1.948197                      -0.113309   0.94   \n",
      "118                  0.659437                       1.861288   0.87   \n",
      "118                  0.659437                       1.861288   0.87   \n",
      "122                  0.224999                      -1.256503   0.20   \n",
      "123                 -1.074024                      -0.738963   0.47   \n",
      "123                 -1.074024                      -0.738963   0.47   \n",
      "116                 -0.549526                      -1.424435   0.44   \n",
      "116                 -0.549526                      -1.424435   0.44   \n",
      "128                 -0.038123                      -0.853424   0.42   \n",
      "128                 -0.038123                      -0.853424   0.42   \n",
      "117                 -1.125536                      -0.429899   0.17   \n",
      "117                 -1.125536                      -0.429899   0.17   \n",
      "115                 -1.483954                      -0.954512   0.11   \n",
      "115                 -1.483954                      -0.954512   0.11   \n",
      "\n",
      "     true_label  softmax_proba  playoff_pred  \n",
      "124           0       0.168762             1  \n",
      "121           1       0.167083             1  \n",
      "127           1       0.158934             1  \n",
      "125           1       0.154237             1  \n",
      "126           1       0.146714             0  \n",
      "119           0       0.109781             0  \n",
      "120           1       0.107354             1  \n",
      "120           1       0.107354             1  \n",
      "118           1       0.100096             1  \n",
      "118           1       0.100096             1  \n",
      "122           0       0.094489             0  \n",
      "123           1       0.067096             0  \n",
      "123           1       0.067096             0  \n",
      "116           0       0.065113             0  \n",
      "116           0       0.065113             0  \n",
      "128           0       0.063824             0  \n",
      "128           0       0.063824             0  \n",
      "117           1       0.049706             0  \n",
      "117           1       0.049706             0  \n",
      "115           0       0.046811             0  \n",
      "115           0       0.046811             0  \n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T12:07:37.039806Z",
     "start_time": "2024-12-13T12:07:36.769766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import the season 11 data\n",
    "teams_df_11 = pd.read_csv('../dataset/Season_11/teams.csv')\n",
    "players_df_11 = pd.read_csv('../dataset/Season_11/players_teams.csv')\n",
    "coaches_df_11 = pd.read_csv('../dataset/Season_11/coaches.csv')\n",
    "\n",
    "#drop columns\n",
    "teams_df_11 = teams_df_11.drop(columns=['lgID', 'name', 'arena', 'franchID'])\n",
    "coaches_df_11 = coaches_df_11.drop(columns=['lgID', 'stint'])\n",
    "players_df_11 = players_df_11.drop(columns=['lgID', 'stint'])\n",
    "\n",
    "# for each player, look for their bioID in player_rolling_features and add the weighted_score, award_score, and post_weighted_score for years 8, 9, and 10. If the player is not found, add 0 for all three columns, and if the player is found but the year is not found, add 0 for the missing year(s).\n",
    "\n",
    "rolling_years = [8, 9, 10]\n",
    "\n",
    "# Initialize new columns in players_df_11 with float64 data type\n",
    "for year in rolling_years:\n",
    "    players_df_11[f'weighted_score_y{year}'] = 0.0\n",
    "    players_df_11[f'award_score_y{year}'] = 0.0\n",
    "    players_df_11[f'post_weighted_score_y{year}'] = 0.0\n",
    "\n",
    "# Iterate over each row in players_df_11\n",
    "for index, row in players_df_11.iterrows():\n",
    "    bioID = row['playerID']\n",
    "\n",
    "    # Filter player_rolling_features for the current player\n",
    "    player_data = player_rolling_features[player_rolling_features['bioID'] == bioID]\n",
    "\n",
    "    # For each rolling year, fetch scores or assign 0 if not available\n",
    "    for year in rolling_years:\n",
    "        year_data = player_data[player_data['year'] == year]\n",
    "        if not year_data.empty:\n",
    "            players_df_11.at[index, f'weighted_score_y{year}'] = float(year_data['weighted_score'].iloc[0])\n",
    "            players_df_11.at[index, f'award_score_y{year}'] = float(year_data['award_score'].iloc[0])\n",
    "            players_df_11.at[index, f'post_weighted_score_y{year}'] = float(year_data['post_weighted_score'].iloc[0])\n",
    "\n",
    "# Print the updated DataFrame for verification\n",
    "print(players_df_11.head())\n",
    "\n",
    "def calculate_player_rolling_features11(df):\n",
    "    \"\"\"\n",
    "    Calculates the 3-year average of weighted_score, award_score, and post_weighted_score\n",
    "    for each player across years 8, 9, and 10.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame with columns:\n",
    "            - playerID\n",
    "            - year\n",
    "            - tmID\n",
    "            - weighted_score_y8, award_score_y8, post_weighted_score_y8\n",
    "            - weighted_score_y9, award_score_y9, post_weighted_score_y9\n",
    "            - weighted_score_y10, award_score_y10, post_weighted_score_y10\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with added rolling average columns:\n",
    "            - weighted_score_rolling_3\n",
    "            - award_score_rolling_3\n",
    "            - post_weighted_score_rolling_3\n",
    "    \"\"\"\n",
    "    # List of the years to include in the rolling calculation\n",
    "    rolling_years = [8, 9, 10]\n",
    "\n",
    "    # Initialize rolling columns\n",
    "    df['weighted_score_rolling_3'] = 0.0\n",
    "    df['award_score_rolling_3'] = 0.0\n",
    "    df['post_weighted_score_rolling_3'] = 0.0\n",
    "\n",
    "    # Calculate the rolling averages\n",
    "    for index, row in df.iterrows():\n",
    "        scores = {\n",
    "            'weighted_score': [],\n",
    "            'award_score': [],\n",
    "            'post_weighted_score': []\n",
    "        }\n",
    "\n",
    "        # Collect scores for years 8, 9, and 10\n",
    "        for year in rolling_years:\n",
    "            for key in scores.keys():\n",
    "                column_name = f'{key}_y{year}'\n",
    "                if column_name in df.columns:\n",
    "                    scores[key].append(row[column_name])\n",
    "\n",
    "        # Calculate the averages ignoring zeros\n",
    "        for key, values in scores.items():\n",
    "            rolling_avg = (\n",
    "                sum(value for value in values if value != 0) / len(values)\n",
    "                if any(value != 0 for value in values)\n",
    "                else 0\n",
    "            )\n",
    "            df.at[index, f'{key}_rolling_3'] = rolling_avg\n",
    "\n",
    "    return df\n",
    "\n",
    "# Calculate rolling features for players_df_11\n",
    "players_df_11 = calculate_player_rolling_features11(players_df_11)\n",
    "\n",
    "#drop old columns\n",
    "players_df_11 = players_df_11.drop(columns=[\n",
    "    'weighted_score_y8', 'award_score_y8', 'post_weighted_score_y8',\n",
    "    'weighted_score_y9', 'award_score_y9', 'post_weighted_score_y9',\n",
    "    'weighted_score_y10', 'award_score_y10', 'post_weighted_score_y10'\n",
    "])\n",
    "\n",
    "# Print the updated DataFrame for verification\n",
    "print(players_df_11.head())\n",
    "\n",
    "# Aggregate player scores by team\n",
    "team_scores = players_df_11.groupby(['year', 'tmID']).agg({\n",
    "    'weighted_score_rolling_3': 'sum',\n",
    "    'award_score_rolling_3': 'sum',\n",
    "    'post_weighted_score_rolling_3': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "## add the columns weighted_score_rolling_3, award_score_rolling_3, and post_weighted_score_rolling_3 to the teams_df_11 DataFrame\n",
    "\n",
    "# Merge the team_scores with teams_df_11\n",
    "team_scores = pd.merge(teams_df_11, team_scores, on=['year', 'tmID'], how='left')\n",
    "\n",
    "print(team_scores)\n",
    "##"
   ],
   "id": "da1541c54a8e4225",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     playerID  year tmID  weighted_score_y8  award_score_y8  \\\n",
      "0  adairje01w    11  MIN                0.0             0.0   \n",
      "1  adamsda01w    11  SAS                0.0             0.0   \n",
      "2  ajavoma01w    11  WAS                0.0             0.0   \n",
      "3  anosini01w    11  WAS                0.0             0.0   \n",
      "4  appelja01w    11  SAS                0.0             0.0   \n",
      "\n",
      "   post_weighted_score_y8  weighted_score_y9  award_score_y9  \\\n",
      "0                     0.0               0.00             0.0   \n",
      "1                     0.0               0.00             0.0   \n",
      "2                     0.0             333.85             0.0   \n",
      "3                     0.0             484.60             0.0   \n",
      "4                     0.0               0.00             0.0   \n",
      "\n",
      "   post_weighted_score_y9  weighted_score_y10  award_score_y10  \\\n",
      "0                     0.0                0.00              0.0   \n",
      "1                     0.0                0.00              0.0   \n",
      "2                     0.0              333.10              0.0   \n",
      "3                     0.0              522.75              0.0   \n",
      "4                     0.0                0.00              0.0   \n",
      "\n",
      "   post_weighted_score_y10  \n",
      "0                     0.00  \n",
      "1                     0.00  \n",
      "2                    22.85  \n",
      "3                     0.00  \n",
      "4                     0.00  \n",
      "     playerID  year tmID  weighted_score_rolling_3  award_score_rolling_3  \\\n",
      "0  adairje01w    11  MIN                  0.000000                    0.0   \n",
      "1  adamsda01w    11  SAS                  0.000000                    0.0   \n",
      "2  ajavoma01w    11  WAS                222.316667                    0.0   \n",
      "3  anosini01w    11  WAS                335.783333                    0.0   \n",
      "4  appelja01w    11  SAS                  0.000000                    0.0   \n",
      "\n",
      "   post_weighted_score_rolling_3  \n",
      "0                       0.000000  \n",
      "1                       0.000000  \n",
      "2                       7.616667  \n",
      "3                       0.000000  \n",
      "4                       0.000000  \n",
      "    year tmID confID  weighted_score_rolling_3  award_score_rolling_3  \\\n",
      "0     11  ATL     EA               2729.800000               3.333333   \n",
      "1     11  CHI     EA               2014.250000               0.000000   \n",
      "2     11  CON     EA               1792.766667               0.000000   \n",
      "3     11  IND     EA               2915.916667               2.333333   \n",
      "4     11  LAS     WE               2797.700000               6.666667   \n",
      "5     11  MIN     WE               2855.000000               2.000000   \n",
      "6     11  NYL     EA               2635.733333               4.666667   \n",
      "7     11  PHO     WE               3469.166667               8.000000   \n",
      "8     11  SAS     WE               2714.066667               0.000000   \n",
      "9     11  SEA     WE               3496.533333              10.666667   \n",
      "10    11  TUL     WE               1506.800000               0.000000   \n",
      "11    11  WAS     EA               2068.816667               1.666667   \n",
      "\n",
      "    post_weighted_score_rolling_3  \n",
      "0                       97.750000  \n",
      "1                       70.550000  \n",
      "2                      110.666667  \n",
      "3                      556.500000  \n",
      "4                      304.850000  \n",
      "5                      181.550000  \n",
      "6                      367.966667  \n",
      "7                      532.816667  \n",
      "8                      354.216667  \n",
      "9                      408.550000  \n",
      "10                      96.483333  \n",
      "11                     102.200000  \n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T12:07:37.063986Z",
     "start_time": "2024-12-13T12:07:37.057448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# look in the coaches_rollings_features for the win_percentage_rolling_3 and post_win_percentage_rolling_3 for each coach in the coaches_df_11 DataFrame and add them to the team_scores DataFrame\n",
    "\"\"\"\n",
    "# Initialize new columns in team_scores with float64 data type\n",
    "team_scores['win_percentage_rolling_3'] = 0.0\n",
    "team_scores['post_win_percentage_rolling_3'] = 0.0\n",
    "\n",
    "# Iterate over each row in team_scores\n",
    "for index, row in team_scores.iterrows():\n",
    "    tmID = row['tmID']\n",
    "\n",
    "    # Filter coaches_rolling_features for the current team\n",
    "    team_data = coaches_rolling_features[coaches_rolling_features['tmID'] == tmID]\n",
    "\n",
    "    # For each rolling year, fetch scores or assign 0 if not available\n",
    "    for year in rolling_years:\n",
    "        year_data = team_data[team_data['year'] == year]\n",
    "        if not year_data.empty:\n",
    "            team_scores.at[index, 'win_percentage_rolling_3'] = float(year_data['win_percentage_rolling_3'].iloc[0])\n",
    "            team_scores.at[index, 'post_win_percentage_rolling_3'] = float(year_data['post_win_percentage_rolling_3'].iloc[0])\n",
    "\n",
    "# Print the updated DataFrame for verification\n",
    "print(team_scores)\n",
    "\"\"\""
   ],
   "id": "2679371056b9e790",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Initialize new columns in team_scores with float64 data type\\nteam_scores['win_percentage_rolling_3'] = 0.0\\nteam_scores['post_win_percentage_rolling_3'] = 0.0\\n\\n# Iterate over each row in team_scores\\nfor index, row in team_scores.iterrows():\\n    tmID = row['tmID']\\n\\n    # Filter coaches_rolling_features for the current team\\n    team_data = coaches_rolling_features[coaches_rolling_features['tmID'] == tmID]\\n\\n    # For each rolling year, fetch scores or assign 0 if not available\\n    for year in rolling_years:\\n        year_data = team_data[team_data['year'] == year]\\n        if not year_data.empty:\\n            team_scores.at[index, 'win_percentage_rolling_3'] = float(year_data['win_percentage_rolling_3'].iloc[0])\\n            team_scores.at[index, 'post_win_percentage_rolling_3'] = float(year_data['post_win_percentage_rolling_3'].iloc[0])\\n\\n# Print the updated DataFrame for verification\\nprint(team_scores)\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T12:07:37.305114Z",
     "start_time": "2024-12-13T12:07:37.266171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Select only the features used for prediction\n",
    "rolling_columns_aggregated = ['award_score_rolling_3', 'weighted_score_rolling_3', 'post_weighted_score_rolling_3']\n",
    "X_new = team_scores[rolling_columns_aggregated].copy()\n",
    "\n",
    "# Preserve the confID for splitting and display purposes\n",
    "X_new['confID'] = team_scores['confID']\n",
    "\n",
    "# Split the data into East and West, using the preserved confID\n",
    "X_new_east = X_new[X_new['confID'] == 'EA']\n",
    "X_new_west = X_new[X_new['confID'] == 'WE']\n",
    "\n",
    "# Standardize the features\n",
    "X_new_east_scaled = scaler.transform(X_new_east[rolling_columns_aggregated])\n",
    "X_new_west_scaled = scaler.transform(X_new_west[rolling_columns_aggregated])\n",
    "\n",
    "# Predict probabilities for the East\n",
    "team_scores_east = team_scores[team_scores['confID'] == 'EA'].copy()\n",
    "team_scores_east['proba'] = clf_east.predict_proba(X_new_east_scaled)[:, 1]\n",
    "\n",
    "# Predict probabilities for the West\n",
    "team_scores_west = team_scores[team_scores['confID'] == 'WE'].copy()\n",
    "team_scores_west['proba'] = clf_west.predict_proba(X_new_west_scaled)[:, 1]\n",
    "\n",
    "# apply softmax to the probabilities of the East\n",
    "team_scores_east['softmax_proba'] = apply_softmax(team_scores_east)\n",
    "\n",
    "# apply softmax to the probabilities of the West\n",
    "team_scores_west['softmax_proba'] = apply_softmax(team_scores_west)\n",
    "\n",
    "# Combine the predictions for both conferences\n",
    "team_scores_combined = pd.concat([team_scores_east, team_scores_west])\n",
    "\n",
    "# multiply softmax_proba by 4\n",
    "team_scores_combined['softmax_proba'] = team_scores_combined['softmax_proba'] * 4\n",
    "\n",
    "#round the softmax_proba to 2 decimal places\n",
    "team_scores_combined['softmax_proba'] = team_scores_combined['softmax_proba'].round(2)\n",
    "\n",
    "# Sort the predictions by conference and probability\n",
    "team_scores_combined = team_scores_combined.sort_values(by=['confID', 'softmax_proba'], ascending=[True, False])\n",
    "\n",
    "# Output the sorted predictions for each conference\n",
    "for confID, group in team_scores_combined.groupby('confID'):\n",
    "    print(f\"Conference {confID} Playoff Predictions:\")\n",
    "    print(group[['year', 'tmID', 'softmax_proba']])\n",
    "    print(\"\\n\")\n",
    "\n",
    "#output to a csv ordered by tmID with the columns tmID and softmax_proba renamed as Playoff\n",
    "team_scores_combined = team_scores_combined.rename(columns={'softmax_proba': 'Playoff'})\n",
    "\n",
    "team_scores_combined = team_scores_combined[['tmID', 'Playoff']].sort_values(by='tmID')\n",
    "\n",
    "# in team_scores_east and team_scores_west rename softmax_proba to Playoff and set the 4 highest probabilities to 1 and the rest to 0\n",
    "team_scores_east = team_scores_east.rename(columns={'softmax_proba': 'Playoff'})\n",
    "team_scores_west = team_scores_west.rename(columns={'softmax_proba': 'Playoff'})\n",
    "\n",
    "# Set the 4 highest probabilities to 1 and the rest to 0 for each conference\n",
    "team_scores_east = team_scores_east.sort_values(by='Playoff', ascending=False)\n",
    "team_scores_east['Playoff'] = [1 if i < 4 else 0 for i in range(len(team_scores_east))]\n",
    "\n",
    "team_scores_west = team_scores_west.sort_values(by='Playoff', ascending=False)\n",
    "team_scores_west['Playoff'] = [1 if i < 4 else 0 for i in range(len(team_scores_west))]\n",
    "\n",
    "# combine the east and west predictions\n",
    "team_scores_combined = pd.concat([team_scores_east, team_scores_west])\n",
    "\n",
    "#sort by tmID\n",
    "team_scores_combined = team_scores_combined.sort_values(by='tmID')\n",
    "\n",
    "#create a dataframe with the columns tmID and Playoff\n",
    "team_scores_final = team_scores_combined[['tmID', 'Playoff']]\n",
    "\n",
    "print(team_scores_final)\n",
    "\n",
    "team_scores_final.to_csv('team_scores.csv', index=False)\n"
   ],
   "id": "89a8fc38dd12cfd2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conference EA Playoff Predictions:\n",
      "    year tmID  softmax_proba\n",
      "3     11  IND           0.96\n",
      "1     11  CHI           0.74\n",
      "2     11  CON           0.64\n",
      "0     11  ATL           0.59\n",
      "11    11  WAS           0.56\n",
      "6     11  NYL           0.52\n",
      "\n",
      "\n",
      "Conference WE Playoff Predictions:\n",
      "    year tmID  softmax_proba\n",
      "7     11  PHO           0.84\n",
      "9     11  SEA           0.81\n",
      "8     11  SAS           0.76\n",
      "4     11  LAS           0.64\n",
      "5     11  MIN           0.47\n",
      "10    11  TUL           0.47\n",
      "\n",
      "\n",
      "   tmID  Playoff\n",
      "0   ATL        1\n",
      "1   CHI        1\n",
      "2   CON        1\n",
      "3   IND        1\n",
      "4   LAS        1\n",
      "5   MIN        0\n",
      "6   NYL        0\n",
      "7   PHO        1\n",
      "8   SAS        1\n",
      "9   SEA        1\n",
      "10  TUL        0\n",
      "11  WAS        0\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T12:07:37.327469Z",
     "start_time": "2024-12-13T12:07:37.325115Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "68962dc65fb6a8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
